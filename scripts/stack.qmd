---
title: "Model stacking with `{stacks}` and `{tidymodels}`"
author: "POLI_SCI 490: Machine Learning in Political Science"
format: html
---

## Setup

Drawing heavily from [here](https://stacks.tidymodels.org/articles/basics.html)

First, we install the required packages.

```{r, warning = FALSE, message = FALSE}
library(tidyverse) # data management
library(tidymodels) # ML workflow
library(stacks) # Model stacking
library(GGally) # Correlation plot
```


## Data

We will work with the `tree_frogs` data from the `{stacks}` package.

```{r}
data("tree_frogs") # Optional step to attach in environment

# subset
# subset the data
tree_frogs = tree_frogs %>% 
  filter(!is.na(latency)) %>% 
  select(-c(clutch, hatched))

tree_frogs %>% head() # explore
```

This is a placeholder code chunk for some additional data exploration

```{r}
# https://r-graph-gallery.com/199-correlation-matrix-with-ggally.html
ggpairs(tree_frogs, progress = FALSE)

# too much going on but an usual place to start
```


We will focus on `latency` as the primary outcome. This is how long the egg takes to hatch after receiving some kind of stimulus. See `help(tree_frogs)` for more.

## Setup

We will try three different models

- KNN
- Linear Regression
- SVM

But first, define some common options across model definitions.

```{r}
# Train-test split
set.seed(20260219)
tree_frogs_split = initial_split(tree_frogs)
tree_frogs_train = training(tree_frogs_split)
tree_frogs_test  = testing(tree_frogs_split)

# Cross-validation
set.seed(20260219)
folds = rsample::vfold_cv(tree_frogs_train, v = 5)

# Recipe
tree_frogs_rec = 
  recipe(latency ~ ., data = tree_frogs_train)

# Error metrics
metric = metric_set(rmse)

# Some of our models involve tuning or resampling
# We need to define these settings to that
# the stacks package knows how the tuning was done
ctrl_grid = control_stack_grid()
ctrl_res = control_stack_resamples()
```

## Models

### KNN (with hyperparameter tuning)

```{r}
# create a model definition
knn_spec =
  nearest_neighbor(
    mode = "regression", 
    neighbors = tune("k")
  ) %>% 
  set_engine("kknn")

# check
knn_spec
```

Then extend the recipe

```{r}
knn_rec =
  tree_frogs_rec %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_impute_mean(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors())

knn_rec
```

Add both to a workflow

```{r}
knn_wflow = 
  workflow() %>% 
  add_model(knn_spec) %>% 
  add_recipe(knn_rec)

knn_wflow
```


Finally, tune and fit

```{r}
set.seed(20260219)
knn_res =
  tune_grid(
    knn_wflow,
    resamples = folds,
    metrics = metric,
    grid = 4,
    control = ctrl_grid
  )

knn_res
```

Now the object is ready for stacking.

### Linear model

No grid for hyperparameters, just resampling.

```{r}
# create a model definition
lin_reg_spec =
  linear_reg() %>% 
  set_engine("lm")

# extend the recipe
# Think a bit about whether we need all pre-processing
# steps the same or not
lin_reg_rec =
  tree_frogs_rec %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors())

# add both to a workflow
lin_reg_wflow = 
  workflow() %>% 
  add_model(lin_reg_spec) %>% 
  add_recipe(lin_reg_rec)

# fit to the 5-fold cv
set.seed(20260219)
lin_reg_res =
  fit_resamples(
    lin_reg_wflow,
    resamples = folds,
    metrics = metric,
    control = ctrl_res
  )

lin_reg_res
```


### Support vector machine

```{r}
# create a model definition
# kernlab package needs to be installed
# talk more about engine and tuning parameters
svm_spec = 
  svm_rbf(
    cost = tune("cost"), 
    rbf_sigma = tune("sigma")
  ) %>% 
  set_engine("kernlab") %>% 
  set_mode("regression")

# extend the recipe
svm_rec =
  tree_frogs_rec %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_impute_mean(all_numeric_predictors()) %>% 
  step_corr(all_predictors()) %>% 
  step_normalize(all_numeric_predictors())

# add both to a workflow
svm_wflow = 
  workflow() %>% 
  add_model(svm_spec) %>% 
  add_recipe(svm_rec)

# tune cost and sigma and fit to the 5-fold cv
set.seed(20260219)
svm_res =
  tune_grid(
    svm_wflow, 
    resamples = folds, 
    grid = 6,
    metrics = metric,
    control = ctrl_grid
  )

svm_res
```

### Candidate models

Note that each different model admits different configurations.

![](https://raw.githubusercontent.com/tidymodels/stacks/main/man/figures/candidates.png)


We have 11 here. In a realistic application you would have many more. Imagine what would happen if the linear model was fit with lasso instead.

## Stacking

We start by creating a data stack object to store our predictions for each ensemble member.

![](https://raw.githubusercontent.com/tidymodels/stacks/main/man/figures/data_stack.png)

You build a stack by literally stacking *candidates* on top of each other.

```{r}
tree_frogs_data_st =
  stacks() %>% 
  add_candidates(knn_res) %>% 
  add_candidates(lin_reg_res) %>% 
  add_candidates(svm_res)

tree_frogs_data_st
```

This is just a tibble (with extra attributes) under the hood

```{r}
as_tibble(tree_frogs_data_st)
```


## Fit and evaluate stack

As you may tell by visual inspection, predictions correlate across members. The `blend_predictions()` function performs regularization. 

The general idea is to use LASSO regularization to determine which candidates become members.

![](https://raw.githubusercontent.com/tidymodels/stacks/main/man/figures/coefs.png)

```{r}
tree_frogs_model_st =
  tree_frogs_data_st %>% 
  blend_predictions()

tree_frogs_model_st
```


We can visualize too

```{r}
autoplot(tree_frogs_model_st)
```

More directly:

```{r}
autoplot(tree_frogs_model_st, type = "members")
```


See the members with highest weights:

```{r}
autoplot(tree_frogs_model_st, type = "weights")
# Aside, these look very different from the tutorial
```

Fit the candidates with non-zero stacking coefficients to the full training set.


```{r}
tree_frogs_model_st =
  tree_frogs_model_st %>% 
  fit_members()

tree_frogs_model_st
```

We can think of this as a group of fitted member models and a set of instructions on how to combine their predictions. In other words, we are not only training each model independently, but also training/learning how to combine them.

![](https://raw.githubusercontent.com/tidymodels/stacks/main/man/figures/class_model_stack.png)

We can look at this more closely:

```{r}
collect_parameters(tree_frogs_model_st, 
                   candidates = "svm_res")
```

## Evaluate

First predict new data

```{r}
tree_frogs_test =
  bind_cols(tree_frogs_test, predict(tree_frogs_model_st, tree_frogs_test))

tree_frogs_test %>% 
  select(latency, .pred)
```

Make a plot

```{r}
ggplot(tree_frogs_test) +
  aes(
    x = latency,
    y = .pred
  ) +
  geom_point() +
  coord_obs_pred()
```


We can also generate predictions for each ensemble member

```{r}
member_preds =
  tree_frogs_test %>% 
  select(latency) %>% 
  bind_cols(predict(tree_frogs_model_st, tree_frogs_test, members = TRUE))

member_preds
```

Calculate RMSE

```{r}
map(member_preds, rmse_vec,
    truth = member_preds$latency) %>%
  as_tibble()
```

