{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV-dLil-LS8t"
      },
      "source": [
        "### Pre-Trained Neural Networks Demo\n",
        "**POLI_SCI 490: Machine Learning in Political Science**\n",
        "*Code adapted from Nora Webb Williams and Hyein Ko*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7y2IM4525Wq"
      },
      "source": [
        "**Source Data: Tensorflow Flower Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tscLMz7O9v30"
      },
      "source": [
        "*   *tensorfolow* is used to build, train, and run deep learning models\n",
        "*   *MobileNetV2* and *ResNet50* are pre-treained image classification models for various computer vision tasks\n",
        "*   *layers, models* help define and structure custom deep learning models\n",
        "*   *PIL (Python Image Library)* is used to open, resize, and process image\n",
        "*   *numpy* helps handle image data as arrays for numerical computation\n",
        "*   *random* helps generate random numbers of make random choices\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ykOFt7p2ykKW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/gustavodiaz/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.utils import class_weight\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tltxLGXZ5_1k"
      },
      "outputs": [],
      "source": [
        "# We will use flower data from tensorflow!\n",
        "# \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TXYuNlLfylRW"
      },
      "outputs": [],
      "source": [
        "# Set seed\n",
        "my_seed = 112358\n",
        "random.seed(my_seed)\n",
        "\n",
        "import os\n",
        "\n",
        "dirname = os.getcwd()\n",
        "\n",
        "\n",
        "# Set file path\n",
        "flower_path = dirname + \"/flower_photos/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GaKGaSRNMsHY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3670 files belonging to 5 classes.\n",
            "Using 2936 files for training.\n",
            "Found 3670 files belonging to 5 classes.\n",
            "Using 734 files for validation.\n",
            "Number of classes: 5\n",
            "List of class: ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n"
          ]
        }
      ],
      "source": [
        "# Set image and batch size\n",
        "# - batch size is the number of training sampled processed at once.\n",
        "#   we use batches because the full dataset may not fit in memory.\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Set training data and validation data\n",
        "train_data = tf.keras.utils.image_dataset_from_directory(\n",
        "    flower_path,\n",
        "    validation_split = 0.2, #using 80% data as training, 20% as validation\n",
        "    subset = \"training\",\n",
        "    seed = my_seed,\n",
        "    image_size = IMG_SIZE,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    label_mode=\"categorical\",\n",
        "    shuffle = True)\n",
        "\n",
        "val_data = tf.keras.utils.image_dataset_from_directory(\n",
        "    flower_path,\n",
        "    validation_split = 0.2,\n",
        "    subset = \"validation\",\n",
        "    seed = my_seed,\n",
        "    image_size = IMG_SIZE,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    label_mode=\"categorical\")\n",
        "\n",
        "# Check classes in the dataset\n",
        "class_names = train_data.class_names\n",
        "num_classes = len(class_names)\n",
        "\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(\"List of class:\", class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6cj6HiGiFTS7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "daisy: 633\n",
            "dandelion: 898\n",
            "roses: 641\n",
            "sunflowers: 699\n",
            "tulips: 799\n"
          ]
        }
      ],
      "source": [
        "# Check whether there is asymmetry in dataset\n",
        "for class_name in class_names:\n",
        "    print(f\"{class_name}: {len(os.listdir(os.path.join(flower_path, class_name)))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yHoCiJUkFwjT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class weights: {0: np.float64(1.1673956262425447), 1: np.float64(0.8121715076071923), 2: np.float64(1.1184761904761904), 3: np.float64(1.0794117647058823), 4: np.float64(0.9160686427457099)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-12 13:30:23.013509: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        }
      ],
      "source": [
        "# Compute class weights\n",
        "y_train = train_data.map(lambda x, y: y).unbatch()\n",
        "y_train = np.array([np.argmax(label.numpy()) for label in y_train])\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "print(\"Class weights:\", class_weights_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8CKEmwixMsMC"
      },
      "outputs": [],
      "source": [
        "# Use the based model from the pre-trained model MobileNetV2\n",
        "# Note: You can also try with ResNet50 or other pre-trained models!\n",
        "base_model = MobileNetV2(input_shape=IMG_SIZE + (3,),\n",
        "                         include_top=False,\n",
        "                         weights='imagenet')\n",
        "\n",
        "# Free the base model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Build the full model\n",
        "model = models.Sequential([\n",
        "    tf.keras.Input(shape=(224, 224, 3)),\n",
        "    tf.keras.layers.Rescaling(1./255),\n",
        "    base_model,\n",
        "    layers.Conv2D(32, 3, activation='relu'),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nGlTp57mMsOo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 309ms/step - accuracy: 0.4309 - loss: 1.3770 - val_accuracy: 0.5817 - val_loss: 1.0395\n",
            "Epoch 2/2\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 343ms/step - accuracy: 0.8019 - loss: 0.6006 - val_accuracy: 0.7602 - val_loss: 0.6851\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "# - epoch is one full pass through the entire training dataset\n",
        "#   one pass is usually not enough for the model to learn well\n",
        "#   but too many can lead to overfitting (the model will memorize the data)\n",
        "# - if you have 100 images and train for 2 epoches,\n",
        "#   the model will see all 100 images 2 times during training\n",
        "\n",
        "EPOCHS = 2\n",
        "history = model.fit(train_data,\n",
        "                    validation_data = val_data,\n",
        "                    epochs = EPOCHS,\n",
        "                    class_weight = class_weights_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FpzUrFAM40e"
      },
      "source": [
        "Let's Fine-tune!\n",
        "- Slightly retrain a pretrained model on our specific dataset to improve performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mCsqxDDnMsRB"
      },
      "outputs": [],
      "source": [
        "# Unfreeze the top layers\n",
        "# - set the last few layers of a pretrained model to be trainable again\n",
        "base_model.trainable = True\n",
        "\n",
        "# Fine tune from this layer onwards\n",
        "fine_tune_at = 100\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable =  False\n",
        "\n",
        "# Complie the model using a much lower training rate\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer = tf.keras.optimizers.Adam(1e-5),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4jBMqs6GPTTH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 327ms/step - accuracy: 0.8728 - loss: 0.4145 - val_accuracy: 0.8120 - val_loss: 0.5290\n",
            "Epoch 2/2\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 331ms/step - accuracy: 0.9103 - loss: 0.2850 - val_accuracy: 0.8406 - val_loss: 0.4481\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "epochs = 2\n",
        "history_fine = model.fit(train_data,\n",
        "                         epochs = epochs,\n",
        "                         validation_data = val_data,\n",
        "                         class_weight = class_weights_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3gt6jUMEMsTf"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model_path = dirname + \"/flower_model.keras\"\n",
        "model.save(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDalWX1gNNLN"
      },
      "source": [
        "Let's predict with a new flower image!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-gXvp57cMsWI"
      },
      "outputs": [],
      "source": [
        "# Load the saved model\n",
        "# - batch_size 64, train_epochs: 10, fine_tune_epochs: 5\n",
        "model = tf.keras.models.load_model(model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JwgtGDXVMsY3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step\n",
            "Predicted flower: roses\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Provide the image\n",
        "img_path = dirname + \"/rose.png\"\n",
        "\n",
        "# Load and preprocess image\n",
        "img = Image.open(img_path).convert(\"RGB\")\n",
        "img = img.resize(IMG_SIZE)\n",
        "img_array = np.array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "img_tensor = tf.convert_to_tensor(img_array, dtype=tf.float32)\n",
        "\n",
        "# Run prediction\n",
        "pred = model.predict(img_tensor)\n",
        "predicted_label = class_names[np.argmax(pred)]\n",
        "\n",
        "# Show result\n",
        "img.show()\n",
        "print(f\"Predicted flower: {predicted_label}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ONq66j7rrJUA",
        "eLct8OV6sSoW",
        "vLweA75eyOFf",
        "tfMo1GyWsY8F",
        "yIR1JR64N0Nk",
        "ePYiwKKBsX5o",
        "j0vszQmX0YOQ",
        "WAGPSRB8aLQz",
        "HE85m4z_aQ-s",
        "AWAGbzU6aSOW"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
