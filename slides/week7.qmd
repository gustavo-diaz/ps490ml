---
title: | 
       | Ensemble Learning
subtitle: "POLI_SCI 490"
author: "Machine Learning in Political Science"
format: 
  revealjs:
    center: true
    slide-number: false
    progress: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE,
                      message = FALSE,
                      fig.pos = "center")

# Packages
library(tidyverse)
library(tidymodels)
library(tinytable)
library(kableExtra)
library(marginaleffects)

# ggplot global options
theme_set(theme_gray(base_size = 20))

# Colors
nu_purple =  "#4E2A84"
```


## Plan for today

**Coding**

- Model stacking

**Discussion**

- Ensemble learning + Reading

# Coding

<https://github.com/gustavo-diaz/ps490ml/blob/main/scripts/stack.qmd>

# Discussion

## Ensemble approaches

1. Bagging (+ random forests)

2. Boosting

3. Bayesian models (e.g. BART)

4. *Stacking*

# Questions

## When do you need an ensemble?

## Which approach to ensembles do you prefer?

## What is better?

1. Fine-tuning a single model
2. Stacking many different kinds of models




